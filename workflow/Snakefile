configfile: "workflow/config/config.yaml"

# Set input file locations
samples_files = config["samples"]
taxids_file = config["taxids"]
Results = config["output_dir"]

# Set user parameters
parallel_processes = config["parallel_processes"]
email = config["email"]
ensembl_release = config["ensembl_release"]
compress = config["compress"]


with open ('taxids.txt', 'r') as tax_read:
    taxids = tax_read.read().splitlines()
with open ('samples.txt', 'r') as sample_read:
    samples = sample_read.read().splitlines()

[x for x in taxids if x.strip()]
[x for x in samples if x.strip()]

ruleorder: Metadata > Srafetch > References > Decompress > Rsem_idx > Fasterqdump > Compress > Rsem_expression


rule all:
    input:
        rsem_genes = expand(Results + "Counts/{taxid}/{sample}_rsem.genes.results", taxid = taxids, sample = samples),
        rsem_isoforms = expand(Results + "Counts/{taxid}/{sample}_rsem.isoforms.results", taxid = taxids, sample = samples)
        

rule Metadata:
    params:
        email = email,
        gsm_id = "{sample}"
    output:
        metadata_files = expand(Results + "Metadata/{sample}.csv", sample = samples)
    conda:
        "envs/entrez_direct.yaml"
    group:
        "Metadata"
    threads:
        max(workflow.cores / 3, 3)  ## Ensure that no more than 3 request pr second due to Entrez-Eutils restrictions
    shell:
        "esearch -db sra -query {params.gsm_id} | efetch -format runinfo > {output.metadata_files} -email {params.email}"


rule Srafetch:
    input:
        metadata_files = rules.Metadata.output.metadata_files
    output:
        sra_files = temp(expand(Results + "Rawdata/SRA/{sample}.sra", sample = samples))
    params:
            cmd = "prefetch"
    conda:
            "envs/sra_tools.yaml"
    group:
        "Samples"
    script:
            "scripts/prefetch.py"


rule References:
    input:
        metadata_files = rules.Srafetch.input.metadata_files  ## Keep expansion, remove comment when below are finished
    params:
        release = ensembl_release,
        taxids = taxids,
        output_dir = Results + "References"
    output:
        gtf_files = pipe(Results + "References/{taxid}.gtf.gz"),   # Remove Expansion
        fa_files = pipe(Results + "References/{taxid}.fa.gz")   # Remove Expansion
    conda:
        "envs/references.yaml"
    group:
        "Genome"
    threads:
        workflow.cores / parallel_processes  ## No multi-threading support
    script:
        "scripts/references.R"

        
rule Decompress:
    input:
        gtf_files = rules.References.output.gtf_files,
        fa_files = rules.References.output.fa_files
    output:
        gtf_files = Results + "References/{taxid}.gtf",  ## Should be temp
        fa_files = Results + "References/{taxid}.fa"  ## Should be temp
    wildcard_constraints:
        taxid = "\d+"
    conda:
        "envs/decompress.yaml"
    group:
        "Genome"
    threads:
        workflow.cores / parallel_processes
    shell:
        "unpigz --force --processes {workflow.cores} {input.gtf_files} {input.fa_files}"


rule Rsem_idx:
    input:
        gtf_files = rules.Decompress.output.gtf_files,
        fa_files = rules.Decompress.output.fa_files
    output:
        idx_dir = directory(Results + "References/{taxid}/Indices/RSEM"),
    conda:
        "envs/rsem.yaml"
    group:
        "Genome"
    threads:
        workflow.cores
    shell:
        "rsem-prepare-reference \
            --gtf {input.gtf_files} \
            --star \
            --num-threads {threads} \
            {input.fa_files} \
            {output.idx_dir}/idx"


rule Fasterqdump:
    input:
        metadata_files = rules.Metadata.output.metadata_files,
        sra_files = rules.Srafetch.output.sra_files
    output:
            fastq_1 = pipe(Results + "Rawdata/Fastq/{sample}_1.fastq"),  ## Should be temp()??
            fastq_2 = pipe(Results + "Rawdata/Fastq/{sample}_2.fastq")   ## Should be temp()??
    params:
            cmd = "fasterq-dump",
            cmd2 = "pigz",
            compress = compress,
            fastq_dir = Results + "Rawdata/Fastq"
    conda:
        "envs/sra_tools.yaml"
    group:
        "Fetch"
    threads:
        workflow.cores / parallel_processes
    script:
        "scripts/fasterq_dump.py"
        
rule Compress:
    input:
        fastq_1 = rules.Fasterqdump.output.fastq_1,
        fastq_2 = rules.Fasterqdump.output.fastq_2
    output:
        fastq_1 = temp(Results + "Rawdata/Fastq/{sample}_1.fastq.gz"),
        fastq_2 = temp(Results + "Rawdata/Fastq/{sample}_2.fastq.gz")
    threads:
        workflow.cores / parallel_processes
    group:
        "Fetch"
    shell:
        "pigz \
            --force \
            --processes {threads} \
            {input.fastq_1} {input.fastq_2}"


rule Rsem_expression:
    input:
        fastq_1 = expand(rules.Compress.output.fastq_1, sample = samples),
        fastq_2 = expand(rules.Compress.output.fastq_2, sample = samples),
        rsem_idx = expand(rules.Rsem_idx.output.idx_dir, taxid = taxids)
    output:
        rsem_genes = expand(Results + "Counts/{taxid}/{sample}_rsem.genes.results", taxid = taxids, sample = samples),
        rsem_isoforms = expand(Results + "Counts/{taxid}/{sample}_rsem.isoforms.results", taxid = taxids, sample = samples)
    params:
        rsem_prefix = Results + "Counts/$species/{sample}_rsem"
    conda:
        "envs/rsem.yaml"
    group:
        "Quantification"
    threads:
        workflow.cores / parallel_processes
    shell:
        "rsem-calculate-expression --paired-end \
            --star \
            --paired-end \
            --num-threads {threads} \
            {input.fastq_1} \
            {input.fastq_2} \
            {params.rsem_idx} \
            {params.rsem_prefix}"

# """
# In case of paired end data, Transcriptome output must be sorted by coordinate,
# as it is a requirement for HTSeq-counts to have data sorted by reads or coordinates
# """
# rule tx_sort:
#     input:
#         csv_file       = metadata_files,
#         tx_bam          = tx_bam
#     output:
#         tx_bam_sorted   = tx_bam_sorted
#         # tx_bam_sorted   = temp(tx_bam_sorted)
#     params:
#         cmd             = samtools_sort
#     conda:
#       "envs/samtools.yaml"
#     threads:
#         ceil(workflow.cores / parallel)
#     log:
#         log_dir + "{sample}.tx_sort.log"
#     benchmark:
#         benchmark_dir + "{sample}.tx_sort.bench"
#     script:
#         "scripts/tx_sort.py"


# """
# makeHTSeqTables extracts counts from the STAR alignment for genes if a single end SRA is used, 
# or genes and transcripts if a paired end SRA is used. 
# """
# rule count_reads:
#     input:
#         csv_file               = metadata_files,
#         gene_bam                = gene_bam,
#         tx_bam_sorted           = tx_bam_sorted
#     output:
#        gene_counts_files        = gene_counts_files,
#        transcript_counts_files  = transcript_counts_files
#     params:
#         cmd                     = htseq_count,
#         ref_dir                 = ref_dir,
#         strandedness            = strandedness
#     conda:
#       "envs/samtools.yaml"
#     threads:
#         ceil(workflow.cores / parallel)
#     log:
#         log_dir + "{species}.{sample}.count_reads.log"
#     benchmark:
#         benchmark_dir + "{species}.{sample}.count_reads.bench"
#     script:
#         "scripts/count_reads.py"


# """
# Samples with high zero inflation could skewer the normalization.
# Zero contents in the samples are screened for zero contents,
# and samples with a higher zero contents than a set threshold are dismissed.
# """
# rule zero_intolerance:
#     input:
#         gene_counts_files           = gene_counts_files,
#         transcript_counts_files     = transcript_counts_files
#     output:
#         gene_filtered_files         = gene_filtered_files,
#         transcript_filtered_files   = transcript_filtered_files
#     params:
#         threshold                   = threshold,
#         threads                     = sub_threads
#     conda:
#         "envs/R.yaml"
#     threads:
#         ceil(workflow.cores / parallel)
#     script:
#         "scripts/zero_intolerance.R"

# # """
# # Normalize samples counts
# # """
# # rule normalize_counts:
# #     input:
# #         gene_filtered_files         = expand(gene_filtered_files, sample = samples, species = species),
# #         transcript_filtered_files   = expand(transcript_filtered_files, sample = samples, species = species)
# #     output:
# #         gene_normalized_files       = gene_normalized_files,
# #         transcript_normalized_files = transcript_normalized_files
# #     params:
# #         threads                     = sub_threads
# #     conda:
# #         "envs/R.yaml"
# #     threads:
# #         round(max_threads / sub_threads) ## Allows simultanous sample processing
# #     script:
# #         "scripts/normalize_counts.R"
