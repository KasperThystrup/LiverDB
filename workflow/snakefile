from snakemake import shell
configfile: "workflow/config/config.yaml" # Remember to direct to correct config file

# Set input file locations
samples_files = config["samples"]
taxids_file = config["taxids"]
Results = config["output_dir"]

# Set user parameters
parallel_processes = config["parallel_processes"]
email = config["email"]
ensembl_release = config["ensembl_release"]
compress = config["compress"]
threshold = config["threshold"]


with open (taxids_file, 'r') as tax_read:
    taxids = tax_read.read().splitlines()
with open (samples_files, 'r') as sample_read:
    samples = sample_read.read().splitlines()

[x for x in taxids if x.strip()]
[x for x in samples if x.strip()]

ruleorder: Metadata > References > Decompress > Rsem_idx > Srafetch > Fasterqdump > Compress


rule all:
    input:
        rsem_genes = expand(Results + "Counts/{taxid}/{sample}_rsem.genes.results", taxid = taxids, sample = samples),
        rsem_isoforms = expand(Results + "Counts/{taxid}/{sample}_rsem.isoforms.results", taxid = taxids, sample = samples),
        sra_file = expand(Results + "Rawdata/SRA/{sample}.sra", sample = samples)


"""
Sample file metadata are collected through Entrez-direct, by querying sample IDs against the sra database.
RunInfo data are requested and saved as .csv files
"""
rule Metadata:
    params:
        sample = "{sample}",
        email = email
    output:
        metadata_file = Results + "Metadata/{sample}.csv"
    conda:
        "envs/entrez_direct.yaml"
    group:
        "Metadata"
    threads:
        workflow.cores
    shell:
        "esearch -db sra -query {params.sample} -email {params.email} | efetch -format runinfo > {output.metadata_file}"


"""
As the user must provide taxids in the taxids.txt file, there is a chance of human errors.
Sample taxids and Scientific namesare collected and compared against the user provided taxids.
As output, a tab-separated file containing taxids, along with the scientific names are provided.
"""
rule Checkspecies:
    input:
        metadata_files = expand(rules.Metadata.output.metadata_file, sample = samples)
    params:
        taxids = taxids
    output:
        detected_species = Results + "Metadata/detectedSpecies.txt"
    group:
        "Genome"
    script:
        "scripts/checkspecies.R"


"""
Using the Scientific names provided from the Checkspecies rule, Ensembl gtf and fasta files are downloaded for each species.
"""
rule References:
    input:
        detected_species = rules.Checkspecies.output.detected_species
    params:
        release = ensembl_release,
    output:
        gtf_file = Results + "References/{taxid}.gtf.gz",   # Remove Expansion
        fa_file = Results + "References/{taxid}.fa.gz"   # Remove Expansion
    conda:
        "envs/references.yaml"
    group:
        "Genome"
    threads:
        workflow.cores / parallel_processes  ## No multi-threading support
    script:
        "scripts/references.R"


"""
Reference files are decompressed using pigz, in order to enable index generation with STAR and possibly also RSEM
"""
rule Decompress:
    input:
        gtf_file = rules.References.output.gtf_file,
        fa_file = rules.References.output.fa_file
    output:
        gtf_file = temp(Results + "References/{taxid}.gtf"),
        fa_file = temp(Results + "References/{taxid}.fa")
    wildcard_constraints:
        taxid = "\d+"
    conda:
        "envs/decompress.yaml"
    group:
        "Genome"
    threads:
        workflow.cores / parallel_processes
    shell:
        "unpigz --force --processes {workflow.cores} {input.gtf_file} {input.fa_file}"


"""
Genome and transcript indices are generated for enabling downstream gene and transcript quantification.
"""
rule Rsem_idx:
    input:
        gtf_file = rules.Decompress.output.gtf_file,
        fa_file = rules.Decompress.output.fa_file      
    output:
        idx_dir = directory(Results + "References/{taxid}/Indices/rsem/"),
        idx_tx = Results + "References/{taxid}/Indices/rsem/rsem.transcripts.fa",
        idx_fa = Results + "References/{taxid}/Indices/rsem/rsem.idx.fa",
        idx_n2g = Results + "References/{taxid}/Indices/rsem/rsem.n2g.idx.fa"        
    conda:
        "envs/rsem.yaml"
    group:
        "Genome"
    threads:
        workflow.cores
    shell:
        "rsem-prepare-reference \
            --gtf {input.gtf_file} \
            --star \
            --num-threads {threads} \
            {input.fa_file} \
            {output.idx_dir}/rsem"


"""
Sample metadata are used to optain SRR numbers for each sample. SRA-tools prefetch are used to download .sra files.
"""
rule Srafetch:
    input:
        metadata_file = rules.Metadata.output.metadata_file
    output:
        sra_file = temp(Results + "Rawdata/SRA/{sample}.sra")
    params:
        cmd = "prefetch"
    conda:
        "envs/sra_tools.yaml"
    group:
        "Samples"
    threads:
        workflow.cores
    script:
        "scripts/prefetch.py"


"""
SRA-tools Fasterq-dump are used to extract Read information into paired-end mate fastq files, enabling multithreaded processing.
"""
rule Fasterqdump:
    input:
        metadata_file = rules.Metadata.output.metadata_file,
        sra_file = rules.Srafetch.output.sra_file
    output:
            fastq_1 = Results + "Rawdata/Fastq/{sample}_1.fastq",
            fastq_2 = Results + "Rawdata/Fastq/{sample}_2.fastq"
    params:
            cmd = "fasterq-dump"
    conda:
        "envs/sra_tools.yaml"
    group:
        "Samples"
    threads:
        workflow.cores
    script:
        "scripts/fasterq_dump.py"


"""
Sample fastq files are compressed, to save disk space.
"""
rule Compress:
    input:
        fastq_1 = rules.Fasterqdump.output.fastq_1,
        fastq_2 = rules.Fasterqdump.output.fastq_2
    output:
        fastq_1 = Results + "Rawdata/Fastq/{sample}_1.fastq.gz",  ##should be temp
        fastq_2 = Results + "Rawdata/Fastq/{sample}_2.fastq.gz"  ##should be temp
    threads:
        workflow.cores / parallel_processes
    conda:
      "envs/decompress.yaml"
    group:
        "Samples"
    shell:
        "pigz \
            --force \
            --processes {threads} \
            {input.fastq_1} {input.fastq_2}"


"""
Gene and transcript quantifications are calculated using RSEM.
"""
rule Rsem_expression:
    input:
        fastq_1 = rules.Compress.output.fastq_1,
        fastq_2 = rules.Compress.output.fastq_2,
    params:
        rsem_prefix = Results + "Counts/{taxid}/{sample}_rsem"
    output:
        rsem_genes = Results + "Counts/{taxid}/{sample}_rsem.genes.results",
        rsem_isoforms = Results + "Counts/{taxid}/{sample}_rsem.isoforms.results"        
    conda:
        "envs/rsem.yaml"
    group:
        "Quantification"
    threads:
        workflow.cores / parallel_processes
    shell:
        "rsem-calculate-expression --paired-end \
            --star \
            --paired-end \
            --num-threads {threads} \
            --star-output-genome-bam \
            --star-gzipped-read-file \
            {input.fastq_1} \
            {input.fastq_2} \
            {input.rsem_idx}/rsem \
            {params.rsem_prefix}"


# """
# Extract count values
# """
# rule Extract_counts:
#     input:
#         rsem_genes = rules.Rsem_expression.output.rsem_genes,
#         rsem_isoforms = rules.Rsem_expression.rsem_isoforms
#     output:
#         genes_count_file = "Counts/{taxid}/{sample}_genes.tsv"
#         transcripts_count_file = "Counts/{taxid}/{sample}_transcripts.tsv"


# """
# Samples with high zero inflation could skewer the normalization.
# Zero contents in the samples are screened for zero contents,
# and samples with a higher zero contents than a set threshold are dismissed.
# """
# rule zero_intolerance:
#     input:
#         genes_count_file =  rules.Extract_counts.output.genes_count_file
#         transcripts_count_file = rules.Extract_counts.output.transcripts_count_file
#     output:
#         genes_count_file = "Counts/{taxid}/{sample}_genes_filtered.tsv"
#         transcripts_count_file = "Counts/{taxid}/{sample}_transcripts_filtered.tsv"
#     params:
#         threshold = threshold,
#         threads = sub_threads
#     conda:
#         "envs/R.yaml"
#     threads:
#         max_threads
#     script:
#         "scripts/zero_intolerance.R"