from snakemake import shell
configfile: "workflow/config/config.yaml" # Remember to direct to correct config file

# Set input file locations
samples_files = config["samples"]
taxids_file = config["taxids"]
Results = config["output_dir"]

# Set user parameters
parallel_processes = config["parallel_processes"]
email = config["email"]
ensembl_release = config["ensembl_release"]
compress = config["compress"]
threshold = config["threshold"]


with open (taxids_file, 'r') as tax_read:
    taxids = tax_read.read().splitlines()
with open (samples_files, 'r') as sample_read:
    samples = sample_read.read().splitlines()

[x for x in taxids if x.strip()]
[x for x in samples if x.strip()]

ruleorder: Metadata > References > Decompress > Rsem_idx > Srafetch > Fasterqdump > Compress > Rsem_expression

## Cleanup
# try: 
#     shell("rm unpaired_samples.txt")
# except all:
#     print("unkown error.. Lazy developper: Please reconfigure try in snakefile [~line 31]!")

rule all:
    input:
        genes_filtered_counts = expand(Results + "Counts/{taxid}/genes_filtered.tsv", taxid = taxids),
        transcripts_filtered_counts = expand(Results + "Counts/{taxid}/transcripts_filtered.tsv", taxid = taxids)

"""
Sample file metadata are collected through Entrez-direct, by querying sample IDs against the sra database.
RunInfo data are requested and saved as .csv files
"""
rule Metadata:
    params:
        sample = "{sample}",
        email = email
    output:
        metadata_file = Results + "Metadata/{sample}.csv"
    conda:
        "envs/entrez_direct.yaml"
    group:
        "Metadata"
    threads:
        workflow.cores # Simultanous jobs causes issues
    shell:
        "esearch -db sra -query {params.sample} -email {params.email} | efetch -format runinfo > {output.metadata_file}"


"""
As the user must provide taxids in the taxids.txt file, there is a chance of human errors.
Sample taxids and Scientific namesare collected and compared against the user provided taxids.
As output, a tab-separated file containing taxids, along with the scientific names are provided.
"""
rule Checkspecies:
    input:
        metadata_files = expand(rules.Metadata.output.metadata_file, sample = samples)
    params:
        taxids = taxids
    output:
        detected_species = Results + "Metadata/detectedSpecies.txt"
    group:
        "Genome"
    threads:
        1
    script:
        "scripts/checkspecies.R"


"""
Using the Scientific names provided from the Checkspecies rule, Ensembl gtf and fasta files are downloaded for each species.
"""
rule References:
    input:
        detected_species = rules.Checkspecies.output.detected_species
    params:
        release = ensembl_release,
    output:
        gtf_file = Results + "References/{taxid}.gtf.gz",   # Remove Expansion
        fa_file = Results + "References/{taxid}.fa.gz"   # Remove Expansion
    conda:
        "envs/references.yaml"
    group:
        "Genome"
    threads:
        workflow.cores / parallel_processes  ## No multi-threading support
    script:
        "scripts/references.R"


"""
Reference files are decompressed using pigz, in order to enable index generation with STAR and possibly also RSEM
"""
rule Decompress:
    input:
        gtf_file = rules.References.output.gtf_file,
        fa_file = rules.References.output.fa_file
    output:
        gtf_file = temp(Results + "References/{taxid}.gtf"),
        fa_file = temp(Results + "References/{taxid}.fa")
    wildcard_constraints:
        taxid = "\d+"
    conda:
        "envs/decompress.yaml"
    group:
        "Genome"
    threads:
        workflow.cores / parallel_processes
    shell:
        "unpigz --force --processes {workflow.cores} {input.gtf_file} {input.fa_file}"


"""
Genome and transcript indices are generated for enabling downstream gene and transcript quantification.
"""
rule Rsem_idx:
    input:
        gtf_file = rules.Decompress.output.gtf_file,
        fa_file = rules.Decompress.output.fa_file      
    output:
        idx_dir = directory(Results + "References/{taxid}/Indices/rsem/"),
        idx_tx = Results + "References/{taxid}/Indices/rsem/rsem.transcripts.fa",
        idx_fa = Results + "References/{taxid}/Indices/rsem/rsem.idx.fa",
        idx_n2g = Results + "References/{taxid}/Indices/rsem/rsem.n2g.idx.fa"        
    conda:
        "envs/rsem.yaml"
    group:
        "Genome"
    threads:
        workflow.cores
    shell:
        "rsem-prepare-reference \
            --gtf {input.gtf_file} \
            --star \
            --num-threads {threads} \
            {input.fa_file} \
            {output.idx_dir}/rsem"


"""
Sample metadata are used to optain SRR numbers for each sample. SRA-tools prefetch are used to download .sra files.
"""
rule Srafetch:
    input:
        metadata_file = rules.Metadata.output.metadata_file
    output:
        sra_file = temp(Results + "Rawdata/SRA/{sample}.sra")
    params:
        cmd = "prefetch"
    conda:
        "envs/sra_tools.yaml"
    group:
        "Samples"
    threads:
        workflow.cores
    script:
        "scripts/prefetch.py"


"""
SRA-tools Fasterq-dump are used to extract Read information into paired-end mate fastq files, enabling multithreaded processing.
"""
rule Fasterqdump:
    input:
        metadata_file = rules.Metadata.output.metadata_file,
        sra_file = rules.Srafetch.output.sra_file
    output:
            fastq = temp(Results + "Rawdata/Fastq/{sample}.fastq"),
            fastq_1 = temp(Results + "Rawdata/Fastq/{sample}_1.fastq"),
            fastq_2 = temp(Results + "Rawdata/Fastq/{sample}_2.fastq")
    params:
            cmd = "fasterq-dump"
    conda:
        "envs/sra_tools.yaml"
    group:
        "Samples"
    threads:
        workflow.cores / parallel_processes # Simultanous jobs may cause issues
    script:
        "scripts/fasterq_dump.py"


"""
Sample fastq files are compressed, to save disk space.
"""
rule Compress:
    input:
        fastq_1 = rules.Fasterqdump.output.fastq_1,
        fastq_2 = rules.Fasterqdump.output.fastq_2
    output:
        fastq_1 = temp(Results + "Rawdata/Fastq/{sample}_1.fastq.gz"),  ##should be temp
        fastq_2 = temp(Results + "Rawdata/Fastq/{sample}_2.fastq.gz")  ##should be temp
    threads:
        workflow.cores / parallel_processes
    conda:
      "envs/decompress.yaml"
    group:
        "Samples"
    shell:
        "pigz \
            --force \
            --processes {threads} \
            {input.fastq_1} {input.fastq_2}"


"""
Gene and transcript quantifications are calculated using RSEM.
"""
rule Rsem_expression:
    input:
        metadata_file = rules.Metadata.output.metadata_file,
        idx_dir = rules.Rsem_idx.output.idx_dir,
        fastq_1 = rules.Fasterqdump.output.fastq_1,
        fastq_2 = rules.Fasterqdump.output.fastq_2
    params:
        cmd = "rsem-calculate-expression",
        rsem_prefix = Results + "Counts/{taxid}/{sample}_rsem"
    output:
        # rsem_prefix = directory(Results + "Counts/{taxid}/RSEM/{sample}_rsem"),
        rsem_genes = Results + "Counts/{taxid}/RSEM/{sample}_rsem.genes.results",
        rsem_isoforms = Results + "Counts/{taxid}/RSEM/{sample}_rsem.isoforms.results"
    params:
        rsem_idx = directory(rules.Rsem_idx.output.idx_dir)
    conda:
        "envs/rsem.yaml"
    group:
        "Quantification"
    threads:
        workflow.cores / parallel_processes
    script:
        "scripts/rsem.py"

"""
Extract count values
"""
rule Extract_counts:
    input:
        rsem_genes = rules.Rsem_expression.output.rsem_genes,
        rsem_isoforms = rules.Rsem_expression.output.rsem_isoforms
    output:
        genes_count_file = Results + "Counts/{taxid}/Samples/{sample}_genes.tsv",
        transcripts_count_file = Results + "Counts/{taxid}/Samples/{sample}_transcripts.tsv"
    threads:
        1
    shell:
        "cut -f1,5 {input.rsem_genes} > {output.genes_count_file} && cut -f1,5 {input.rsem_isoforms} > {output.transcripts_count_file}"


"""
Merge count matrices and dismiss samples with zero contents which exceeds the threshold percentage.
Samples which has an zero count average which exceeds the threshold set in config, will be sorted out.
"""
rule Zero_intolerance:
    input:
        genes_count_files = expand(rules.Extract_counts.output.genes_count_file, taxid = taxids, sample = samples),
        transcripts_count_files = expand(rules.Extract_counts.output.transcripts_count_file, taxid = taxids, sample = samples)
    params:
        threshold = threshold
    output:
        genes_filtered_counts = Results + "Counts/{taxid}/genes_filtered.tsv",
        transcripts_filtered_counts = Results + "Counts/{taxid}/transcripts_filtered.tsv"
    conda:
        "envs/R.yaml"
    threads:
        1
    script:
        "scripts/zero_intolerance.R"

# """
# Samples are normalized using the methodology from DEseq2. 
# """
# rule Normalize:
#     input:
#         genes_filtered_counts = ,
#         transcripts_filtered_counts = 
#     output:
#         genes_normalized_count = ,
#         transcripts_normalized_counts = 
#     conda:
#         "envs/R.yaml"
#     threads:
#         1
#     script:
#         "scripts/normalize.R"